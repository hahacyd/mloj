import numpy as np

# 逻辑回归
class tiny_logistic_regression(object):
    def __init__(self):
        #W
        self.coef_ = None
        #b
        self.intercept_ = None
        #所有的W和b
        self._theta = None
        #01到标签的映射
        self.label_map = {}


    def _sigmoid(self, x):
        return 1. / (1. + np.exp(-x))


    #训练，train_labels中的值可以为任意数值
    def fit(self, train_datas, train_labels, learning_rate=1e-4, n_iters=1e3):
        #loss
        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b.dot(theta))
            try:
                return -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat)) / len(y)
            except:
                return float('inf')

        # 算theta对loss的偏导
        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(y)

        # 批量梯度下降
        def gradient_descent(X_b, y, initial_theta, leraning_rate, n_iters=1e2, epsilon=1e-6):
            theta = initial_theta
            cur_iter = 0
            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - leraning_rate * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break
                cur_iter += 1
            return theta

        unique_labels = list(set(train_labels))
        labels = np.array(train_labels)

        # 将标签映射成0，1
        self.label_map[0] = unique_labels[0]
        self.label_map[1] = unique_labels[1]

        for i in range(len(train_labels)):
            if train_labels[i] == self.label_map[0]:
                labels[i] = 0
            else:
                labels[i] = 1

        X_b = np.hstack([np.ones((len(train_datas), 1)), train_datas])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, labels, initial_theta, learning_rate, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self


    #预测X中每个样本label为1的概率
    def predict_proba(self, X):
        X_b = np.hstack([np.ones((len(X), 1)), X])
        return self._sigmoid(X_b.dot(self._theta))

    #预测
    def predict(self, X):
        proba = self.predict_proba(X)
        result = np.array(proba >= 0.5, dtype='int')
        # 将0，1映射成标签
        for i in range(len(result)):
            if result[i] == 0:
                result[i] = self.label_map[0]
            else:
                result[i] = self.label_map[1]
        return result



class OvO(object):
    def __init__(self):
        # 用于保存训练时各种模型的list
        self.models = []


    def fit(self, train_datas, train_labels):
        '''
        OvO的训练阶段，将模型保存到self.models中
        :param train_datas: 训练集数据，类型为ndarray
        :param train_labels: 训练集标签，标签值为0,1,2之类的整数，类型为ndarray，shape为(-1,)
        :return:None
        '''

        #********* Begin *********#
        label_num = len(set(train_labels))
        classifier_num = label_num * (label_num - 1) / 2

        # 将训练样本分成多个类别
        unique_labels = list(set(train_labels))
        label_samples = {}
        for i in unique_labels:
            label_samples[i] = train_datas[train_labels == i]
        for i in range(label_num):
            j = i+1
            while j < label_num:
                lr = tiny_logistic_regression()
                datas = np.vstack([label_samples[unique_labels[i]], label_samples[unique_labels[j]]])
                lable0 = np.ones(len(label_samples[unique_labels[i]]))
                lable1 = np.ones(len(label_samples[unique_labels[j]]))
                lable0 *= unique_labels[i]
                lable1 *= unique_labels[j]
                lable = np.hstack([lable0,lable1])
                lr.fit(datas, lable)
                self.models.append(lr)
                j += 1
        assert(len(self.models) == classifier_num)
        #********* End *********#


    def predict(self, test_datas):
        '''
        OvO的预测阶段
        :param test_datas:测试集数据，类型为ndarray
        :return:预测结果，类型为ndarray
        '''

        #********* Begin *********#
        result = []
        result_label = np.zeros(test_datas.shape[0])
        for i in self.models:
            y = i.predict(test_datas)
            result.append(y)
        result = np.array(result)
        for i in range(len(result_label)):
            result_label[i] = np.bincount(result[:, i]).argmax()
        return result_label
        #********* End *********#

if __name__ == "__main__":
    train_datas = np.array([[-0.59919456, -0.11866664,  0.36009734,  0.31743078],
                  [-1.57913797,  0.36404513, -1.47491924, -1.43236433],
                  [ 0.99321348, -1.32544606,  1.12962042,  0.72122965],
                  [-1.2116592 , -0.11866664, -1.47491924, -1.43236433],
                  [-1.08916627, -1.80815783, -0.35023166, -0.35556734],
                  [ 0.99321348,  0.60540101,  1.07042634,  1.66342702],
                  [ 1.60567811,  0.36404513,  1.24800859,  0.72122965],
                  [-0.10922286, -1.08409018,  0.06412692, -0.08636809],
                  [ 0.99321348,  0.12268924,  0.47848551,  0.31743078],
                  [-0.96667334,  1.81218043, -1.35653108, -1.43236433],
                  [ 2.21814274, -0.11866664,  1.30720267,  1.39422777],
                  [-1.08916627,  0.84675689, -1.41572516, -1.43236433],
                  [ 0.99321348,  0.12268924,  0.30090326,  0.18283116],
                  [-0.59919456,  1.57082455, -1.41572516, -1.43236433],
                  [ 0.6257347 , -0.60137841,  1.01123226,  1.25962815],
                  [-1.94661675, -0.11866664, -1.65250149, -1.56696396],
                  [-0.23171579,  3.26031574, -1.41572516, -1.16316509],
                  [ 0.01327007, -0.11866664,  0.18251509,  0.31743078],
                  [ 0.25825592, -0.11866664,  0.41929142,  0.18283116],
                  [ 0.74822762, -0.11866664,  0.95203817,  0.72122965],
                  [-0.84418042,  1.08811278, -1.41572516, -1.43236433],
                  [-0.35420871, -1.32544606,  0.00493284, -0.22096772],
                  [ 2.21814274, -1.08409018,  1.78075534,  1.39422777],
                  [ 0.50324177,  0.84675689,  1.01123226,  1.5288274 ],
                  [ 0.38074885, -0.36002253,  0.24170917,  0.04823153],
                  [-0.23171579, -0.36002253,  0.18251509,  0.04823153],
                  [-1.08916627,  1.08811278, -1.35653108, -0.89396584],
                  [-0.35420871, -0.36002253, -0.17264941,  0.04823153],
                  [ 0.13576299, -2.04951372,  0.65606776,  0.31743078],
                  [-0.47670164, -1.08409018,  0.30090326, -0.08636809],
                  [-1.57913797,  0.84675689, -1.47491924, -1.29776471],
                  [-0.35420871, -0.11866664,  0.12332101,  0.04823153],
                  [-0.96667334,  0.84675689, -1.41572516, -1.43236433],
                  [ 0.6257347 , -0.36002253,  0.24170917,  0.04823153],
                  [ 1.1157064 , -0.11866664,  0.95203817,  1.12502853],
                  [ 0.99321348, -0.11866664,  0.65606776,  0.58663003],
                  [ 0.74822762,  0.36404513,  0.71526184,  0.9904289 ],
                  [ 0.13576299,  0.84675689,  0.36009734,  0.4520304 ],
                  [-1.2116592 ,  0.12268924, -1.41572516, -1.56696396],
                  [-1.2116592 , -1.32544606,  0.36009734,  0.58663003],
                  [ 1.23819933,  0.36404513,  1.07042634,  1.39422777],
                  [-0.59919456,  0.84675689, -1.41572516, -1.16316509],
                  [-0.47670164, -1.56680195, -0.05426124, -0.22096772],
                  [ 0.50324177,  0.60540101,  1.24800859,  1.66342702],
                  [-1.08916627,  1.32946866, -1.47491924, -1.43236433],
                  [ 0.25825592, -0.11866664,  0.59687367,  0.72122965],
                  [ 0.74822762, -0.60137841,  0.41929142,  0.31743078],
                  [ 0.01327007, -0.11866664,  0.71526184,  0.72122965],
                  [ 0.50324177, -1.32544606,  0.59687367,  0.31743078],
                  [-1.2116592 ,  0.12268924, -1.41572516, -1.56696396],
                  [ 0.25825592, -1.08409018,  1.01123226,  0.18283116],
                  [ 0.6257347 ,  0.36404513,  0.36009734,  0.31743078],
                  [ 1.48318518, -0.11866664,  1.18881451,  1.12502853],
                  [-0.47670164,  1.08811278, -1.53411333, -1.43236433],
                  [ 0.6257347 , -0.8427343 ,  0.83365001,  0.85582928],
                  [-0.47670164, -1.80815783,  0.06412692,  0.04823153],
                  [ 1.23819933,  0.12268924,  0.89284409,  1.12502853],
                  [-0.10922286, -0.8427343 ,  0.00493284, -0.08636809],
                  [-0.10922286, -0.8427343 ,  0.71526184,  0.85582928],
                  [ 0.25825592, -0.60137841,  0.06412692,  0.04823153],
                  [ 0.01327007,  0.36404513,  0.53767959,  0.72122965],
                  [ 0.38074885,  0.84675689,  0.89284409,  1.39422777],
                  [-0.23171579, -0.60137841,  0.12332101,  0.04823153],
                  [ 0.87072055, -0.11866664,  0.30090326,  0.18283116],
                  [-0.10922286, -0.60137841,  0.71526184,  1.5288274 ],
                  [ 0.13576299, -2.04951372,  0.06412692, -0.35556734],
                  [-0.10922286, -0.8427343 ,  0.12332101, -0.35556734],
                  [-1.57913797,  1.32946866, -1.71169558, -1.43236433],
                  [ 0.99321348,  0.12268924,  1.01123226,  1.5288274 ],
                  [-0.96667334,  0.60540101, -1.29733699, -1.02856546],
                  [ 1.1157064 , -0.60137841,  0.53767959,  0.18283116],
                  [-0.72168749,  1.57082455, -1.41572516, -1.43236433],
                  [-0.84418042, -0.8427343 ,  0.00493284,  0.18283116],
                  [ 0.25825592, -0.60137841,  0.47848551, -0.08636809],
                  [-0.47670164, -1.32544606,  0.06412692,  0.04823153],
                  [-0.47670164, -1.56680195, -0.11345533, -0.35556734],
                  [-0.96667334,  1.08811278, -1.47491924, -1.29776471],
                  [ 0.99321348, -0.11866664,  0.77445592,  1.39422777],
                  [ 0.13576299, -0.11866664,  0.53767959,  0.72122965],
                  [-0.23171579, -0.11866664,  0.18251509, -0.08636809],
                  [-0.96667334, -1.32544606, -0.52781391, -0.22096772],
                  [-0.23171579, -1.08409018, -0.23184349, -0.35556734],
                  [-1.57913797,  0.12268924, -1.41572516, -1.43236433],
                  [-0.84418042,  0.84675689, -1.47491924, -1.43236433],
                  [ 1.23819933,  0.12268924,  0.71526184,  1.39422777],
                  [-0.35420871, -0.60137841,  0.59687367,  0.9904289 ],
                  [-0.47670164,  2.77760397, -1.47491924, -1.43236433],
                  [-1.33415212, -0.11866664, -1.47491924, -1.29776471],
                  [-1.82412383,  0.36404513, -1.53411333, -1.43236433],
                  [ 0.6257347 ,  0.36404513,  0.83365001,  1.39422777],
                  [-1.08916627,  1.08811278, -1.53411333, -1.29776471],
                  [-0.84418042,  2.53624808, -1.41572516, -1.56696396],
                  [ 0.50324177, -1.32544606,  0.65606776,  0.85582928],
                  [ 1.1157064 ,  0.36404513,  1.18881451,  1.39422777],
                  [-0.35420871, -0.11866664,  0.36009734,  0.31743078],
                  [-1.08916627,  0.60540101, -1.47491924, -1.43236433],
                  [-1.82412383, -0.36002253, -1.47491924, -1.43236433],
                  [-1.33415212,  0.12268924, -1.35653108, -1.43236433],
                  [ 2.21814274,  1.81218043,  1.66236717,  1.25962815],
                  [ 0.50324177,  0.60540101,  0.47848551,  0.4520304 ],
                  [-0.96667334,  1.57082455, -1.41572516, -1.16316509],
                  [-0.96667334,  1.08811278, -1.47491924, -1.43236433],
                  [ 2.46312859,  1.81218043,  1.48478492,  0.9904289 ],
                  [ 1.23819933,  0.12268924,  0.59687367,  0.31743078],
                  [ 0.38074885, -0.60137841,  0.53767959,  0.72122965],
                  [-1.82412383, -0.11866664, -1.53411333, -1.43236433],
                  [-0.96667334,  1.81218043, -1.17894883, -1.16316509],
                  [ 0.74822762, -0.11866664,  0.77445592,  0.9904289 ],
                  [-1.2116592 , -1.56680195, -0.35023166, -0.35556734],
                  [-0.23171579, -0.60137841,  0.36009734,  0.04823153],
                  [-0.35420871, -0.8427343 ,  0.18251509,  0.04823153],
                  [ 1.60567811,  1.32946866,  1.30720267,  1.66342702],
                  [ 0.50324177, -0.36002253,  1.01123226,  0.72122965],
                  [ 0.87072055, -0.36002253,  0.41929142,  0.04823153],
                  [ 0.6257347 ,  0.12268924,  0.95203817,  0.72122965],
                  [ 1.36069226,  0.36404513,  0.47848551,  0.18283116],
                  [ 0.50324177, -1.80815783,  0.30090326,  0.04823153],
                  [ 0.74822762, -0.11866664,  1.12962042,  1.25962815],
                  [ 1.60567811, -0.11866664,  1.12962042,  0.4520304 ],
                  [2.21814274, -0.60137841, 1.66236717, 0.9904289]])
    train_labels = np.array([1 ,0, 2, 0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,
 1, 0, 2,2, 0, 1,2,0,2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0,1, 0,1, 1,
 1, 1, 0,2, 2, 1,1,1,0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2, 1, 2, 0, 0,2, 1,1, 1,
 2, 2, 1, 2, 1, 1, 2, 2, 2])
    lr = OvO()
    lr.fit(train_datas,train_labels)


